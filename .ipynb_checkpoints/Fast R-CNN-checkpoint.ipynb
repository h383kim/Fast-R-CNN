{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39667224-9ed1-4a8f-a5d5-74563dcfa991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import VOCDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39547d68-48e6-401a-b889-1ce6fd4a9a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtrainval_06-Nov-2007.tar to ./data\n",
      "Using downloaded and verified file: ./data/VOCtest_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtest_06-Nov-2007.tar to ./data\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VOCDetection(root=\"./data\",\n",
    "                             year=\"2007\",\n",
    "                             download=True,\n",
    "                             image_set=\"trainval\",\n",
    "                             transform=None)\n",
    "test_dataset = VOCDetection(root=\"./data\",\n",
    "                             year=\"2007\",\n",
    "                             download=True,\n",
    "                             image_set=\"test\",\n",
    "                             transform=None)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6382b536-fd5e-4673-8bc1-60bd37ab6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a77e4f40-ccb5-4d1e-994d-9547bba9e4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotation': {'folder': 'VOC2007',\n",
       "  'filename': '000007.jpg',\n",
       "  'source': {'database': 'The VOC2007 Database',\n",
       "   'annotation': 'PASCAL VOC2007',\n",
       "   'image': 'flickr',\n",
       "   'flickrid': '194179466'},\n",
       "  'owner': {'flickrid': 'monsieurrompu', 'name': 'Thom Zemanek'},\n",
       "  'size': {'width': '500', 'height': '333', 'depth': '3'},\n",
       "  'segmented': '0',\n",
       "  'object': [{'name': 'car',\n",
       "    'pose': 'Unspecified',\n",
       "    'truncated': '1',\n",
       "    'difficult': '0',\n",
       "    'bndbox': {'xmin': '141', 'ymin': '50', 'xmax': '500', 'ymax': '330'}}]}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25bed63e-49e5-4f23-87d4-1e24d2c7197c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4952"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a17751-2d3b-42d3-9339-13b270bc404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "#import selective_search\n",
    "\n",
    "import cv2\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b4b1f-c895-482f-b715-e4fa771ecc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def ss_config(ss, img, mode):\n",
    "    ss.setBaseImage(img)\n",
    "\n",
    "    if mode == 's':\n",
    "        ss.switchToSingleStrategy()\n",
    "    elif mode == 'f':\n",
    "        ss.switchToSelectiveSearchFast()\n",
    "    elif mode == 'q':\n",
    "        ss.switchToSelectiveSearchQuality()\n",
    "    else:\n",
    "        print(\"Re-enter the mode. s or f or q\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def selective_search(img, mode='q'):\n",
    "    # Initiate Selective Search\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    # Configure the mode and image\n",
    "    ss_config(ss, img, mode)\n",
    "\n",
    "    # Process Selective-Search\n",
    "    bboxes = ss.process() # bboxes: listof [x, y, w, h]\n",
    "    bboxes[:, 2] += bboxes[:, 0] # bboxes -> listof [x, y, x + w, h]\n",
    "    bboxes[:, 3] += bboxes[:, 1] # bboxes -> listof [x, y, x + w, h + y]\n",
    "\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172800c-4d0f-49d7-8d3c-3870d2073014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_boxes(root):\n",
    "    bboxes=[]\n",
    "    for obj in root.findall('object'):\n",
    "        cls_name = obj.find('name').text\n",
    "        cls_idx = VOC_CLASSES.index(cls_name)\n",
    "        xmin = int(obj.find('bndbox').find('xmin').text)\n",
    "        ymin = int(obj.find('bndbox').find('ymin').text)\n",
    "        xmax = int(obj.find('bndbox').find('xmax').text)\n",
    "        ymax = int(obj.find('bndbox').find('ymax').text)\n",
    "        bboxes.append([xmin, ymin, ymin, ymax, cls_idx]) \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e6078-63fa-492f-b874-3d9179d9011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxA, boxB):\n",
    "    # Compute the Intersection over Union (IoU) between two bounding boxes\n",
    "    xA = max(boxA[0], boxB[0])  # max of xmin\n",
    "    yA = max(boxA[1], boxB[1])  # max of ymin\n",
    "    xB = min(boxA[2], boxB[2])  # min of xmax\n",
    "    yB = min(boxA[3], boxB[3])  # min of ymax\n",
    "\n",
    "    # Compute the area of the intersection rectangle\n",
    "    interWidth = max(0, xB - xA + 1)\n",
    "    interHeight = max(0, yB - yA + 1)\n",
    "    interArea = interWidth * interHeight\n",
    "\n",
    "    # Compute the area of both bounding boxes\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    # Compute the Intersection over Union by dividing the intersection area by the sum of both areas minus the intersection\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea) if interArea > 0 else 0.0\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57f6fe-f35e-402e-9454-675a59aad1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CLASSES = [\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', \n",
    "    'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', \n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701785ca-6a00-48a5-9afd-0e0fa0c55ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, img_set='trainval', transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.img_set = img_set\n",
    "        \n",
    "        self.annotation_path = os.path.join(self.root, f'VOC{self.img_set}_06-Nov-2007', 'VOCdevkit', 'VOC2007', 'Annotations')\n",
    "        self.img_path = os.path.join(self.root, f'VOC{self.img_set}_06-Nov-2007', 'VOCdevkit', 'VOC2007', 'JPEGImages')\n",
    "        self.annotations = [os.path.join(self.annotation_path, xml) for xml in sorted(os.listdir(self.annotation_path)) if not xml.startswith('.')]\n",
    "        self.images = [os.path.join(self.img_path, xml) for xml in sorted(os.listdir(self.img_path)) if not xml.startswith('.')] \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image = cv2.imread(self.images[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # Load annotation\n",
    "        tree = ET.parse(self.annotations[idx])\n",
    "        gt_bboxes_labels = parse_xml_boxes(tree.getroot())\n",
    "        gt_bboxes_labels = np.array(gt_bboxes_labels)\n",
    "        bboxes, labels= gt_bboxes_labels[:, :4], gt_bboxes_labels[:, -1]\n",
    "        # Apply transformation if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Convert data to tensors\n",
    "        #image = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        bboxes = torch.tensor(bboxes).float()\n",
    "        labels = torch.tensor(labels).long()\n",
    "        \n",
    "        return image, bboxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3818bd-a4a2-4e96-b22d-4580a1b85cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import roi_pool\n",
    "\n",
    "vgg = models.vgg16_bn()\n",
    "vgg.load_state_dict(torch.load('/kaggle/input/vgg16/pytorch/default/1/vgg16_bn.pth'))\n",
    "\n",
    "class FastRCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        # vgg = models.vgg16(weights=\"IMAGENET1K_V1\")\n",
    "        # Convolutional layers\n",
    "        self.features = vgg.features\n",
    "        # ROI Pooling\n",
    "        self.roi = roi_pool\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            # nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        self.softmax = nn.Linear(4096, num_classes + 1)\n",
    "        self.bbox = nn.Linear(4096, num_classes * 4)\n",
    "        \n",
    "    def forward(self, images, ROIs):\n",
    "        # Compute feature maps\n",
    "        feature_maps = self.features(images)\n",
    "        # Apply ROI pooling\n",
    "        pooled_features = self.roi(feature_maps, ROIs, output_size=(7, 7), spatial_scale=1.0/16.0)\n",
    "        # Flatten pooled features\n",
    "        x = pooled_features.view(pooled_features.size(0), -1) # .size(0) = Batch Size(N)\n",
    "        # Classifier\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        # Outputs\n",
    "        cls_score = self.cls_score(x) # (N, num_classes)\n",
    "        bbox_pred = self.bbox(x) # (N, num_classes * 4)\n",
    "        \n",
    "        return cls_score, bbox_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00ba65-aadb-4711-b913-b4974ae586ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, lambda_reg=1):\n",
    "        super().__init()__()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.cls_loss = F.cross_entropy()\n",
    "    \n",
    "    def forward(self, cls_preds, bbox_preds, labels, bbox_targets):\n",
    "        # Cross entropy loss for classification\n",
    "        cls = self.cls_lss(cls_preds, labels)\n",
    "        # Bounding box loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a75bfa4-dde6-40c0-9d97-2b7dc2f5ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root_dir = '/kaggle/input/pascal-voc-2007'\n",
    "# Create dataset and dataloader\n",
    "train_dataset = CustomDataset(root_dir, img_set=\"trainval\", transform=None)\n",
    "test_dataset = CustomDataset(root_dir, img_set=\"test\", transform=None)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56209873-cbd7-4d46-93e9-cead6e3cce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8794b04b-4c9c-4361-aeed-96b752713fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fastrcnn = FastRCNN(20).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9582472-3e8f-4fb3-91bd-ba7913c1a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_proposals(proposals, gt_boxes, gt_labels):\n",
    "    threshold_up = 0.5\n",
    "    threshold_low = 0.1\n",
    "    POS_NUM = 16\n",
    "    NEG_NUM = 48\n",
    "    positive_proposals, positive_labels, negative_proposals = [], [], []\n",
    "    \n",
    "    for proposal in proposals:\n",
    "        best_iou = 0\n",
    "        best_label = None\n",
    "        \n",
    "        for gt_box, gt_label in zip(gt_boxes, gt_labels):\n",
    "            iou = compute_iou(proposal, gt_box)\n",
    "            if iou > max_iou:\n",
    "                best_iou = iou\n",
    "                best_label = label\n",
    "        # If the proposal has IOU > 0.5\n",
    "        if best_iou > threshold_up:\n",
    "            positive_proposals.append(proposal)\n",
    "            positivie_labels.append(best_label)\n",
    "        # Else if the proposal has 0.1 < IOU <= 0.5\n",
    "        elif best_iou > threshold_low:\n",
    "            negative_proposals.append(proposal)\n",
    "            # Doesn't need to keep track of label, because background label is fixed\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    # Choose ~16 positives and 48 negatives (More negatives if postives is less than 16)\n",
    "    if len(positive_proposals) < POS_NUM:\n",
    "        POS_NUM = len(positive_proposals)\n",
    "        NEG_NUM = (64 - POS_NUM)\n",
    "    \n",
    "    pos_indices = random.sample(range(len(positive_proposals)), POS_NUM)\n",
    "    neg_indices = random.sample(range(len(negative_proposals)), NEG_NUM)\n",
    "    # Sampling takes place (Replacing the object and all its references if any)\n",
    "    positive_proposals[:] = [positive_proposals[i] for i in pos_indices]\n",
    "    positive_labels[:] = [positive_labels[i] for i in pos_indices]\n",
    "    negative_proposals[:] = [negative_proposals[i] for i in neg_indicies]\n",
    "    \n",
    "    total_proposals = torch.stack(positive_proposals, negative_proposals)\n",
    "    total_labels = positive.labels.extend([20] * NEG_NUM) # Background class label as 20\n",
    "    total_labels = torch.tensor(total_labels, dtype=torch.int64)\n",
    "    return total_proposals, total_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776204a7-7995-49b2-a74c-82eda48c69c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "def train(model, dataloader, num_epochs, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    train_loss, correct, train_acc = 0, 0, 0\n",
    "    total_proposals = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for image, boxes, labels in dataloader:\n",
    "            image = image.to(DEVICE)\n",
    "            boxes = boxes.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            # Generate ROIs and choose 64 proposals(16 positive + 48 negative)\n",
    "            ROIs = selective_search(image)\n",
    "            ROIs, ROI_labels = sample_proposals(ROIs)\n",
    "            ROIs = ROIs.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            cls_scores, bbox_preds = model(image, ROIs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(cls_scores, bbox_preds, labels, bbox_targets)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            pred = torch.argmax(cls_scores, dim=1)\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            total_proposals += 64\n",
    "    \n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc = 100. * correct / total_proposals\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b4373-d57c-479b-b4a2-98b1135f0277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def fine_tune(model: torch.nn.Module, \n",
    "              train_dataloader: torch.utils.data.DataLoader, \n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              loss_fn: torch.nn.Module=nn.CrossEntropyLoss(),\n",
    "              num_epochs: int=10):\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start_time = time.time()\n",
    "        # Train the model and print save the results\n",
    "        train_loss, train_acc = train(model=model,\n",
    "                                      dataloader=train_dataloader, \n",
    "                                      optimizer=optimizer,\n",
    "                                      loss_fn=loss_fn)\n",
    "        \n",
    "        if train_acc > best_acc:\n",
    "            best_acc = train_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        end_time = time.time()\n",
    "        time_elapsed = end_time - start_time\n",
    "        print(f\"------------ epoch {epoch} ------------\")\n",
    "        print(f\"Train loss: {train_loss:.4f} | Train acc: {train_acc:.2f}%\")\n",
    "        print(f\"Time taken: {time_elapsed / 60:.0f}min {time_elapsed % 60:.0f}s\")\n",
    "        \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

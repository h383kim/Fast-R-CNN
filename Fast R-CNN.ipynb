{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39667224-9ed1-4a8f-a5d5-74563dcfa991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import VOCDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39547d68-48e6-401a-b889-1ce6fd4a9a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtrainval_06-Nov-2007.tar to ./data\n",
      "Using downloaded and verified file: ./data/VOCtest_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtest_06-Nov-2007.tar to ./data\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VOCDetection(root=\"./data\",\n",
    "                             year=\"2007\",\n",
    "                             download=True,\n",
    "                             image_set=\"trainval\",\n",
    "                             transform=None)\n",
    "test_dataset = VOCDetection(root=\"./data\",\n",
    "                             year=\"2007\",\n",
    "                             download=True,\n",
    "                             image_set=\"test\",\n",
    "                             transform=None)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6382b536-fd5e-4673-8bc1-60bd37ab6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a77e4f40-ccb5-4d1e-994d-9547bba9e4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotation': {'folder': 'VOC2007',\n",
       "  'filename': '000007.jpg',\n",
       "  'source': {'database': 'The VOC2007 Database',\n",
       "   'annotation': 'PASCAL VOC2007',\n",
       "   'image': 'flickr',\n",
       "   'flickrid': '194179466'},\n",
       "  'owner': {'flickrid': 'monsieurrompu', 'name': 'Thom Zemanek'},\n",
       "  'size': {'width': '500', 'height': '333', 'depth': '3'},\n",
       "  'segmented': '0',\n",
       "  'object': [{'name': 'car',\n",
       "    'pose': 'Unspecified',\n",
       "    'truncated': '1',\n",
       "    'difficult': '0',\n",
       "    'bndbox': {'xmin': '141', 'ymin': '50', 'xmax': '500', 'ymax': '330'}}]}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25bed63e-49e5-4f23-87d4-1e24d2c7197c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4952"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a69f9f32-ce48-4121-9259-2f2792232b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selectivesearch\n",
      "  Downloading selectivesearch-0.4.tar.gz (3.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/h383kim/miniforge3/envs/env/lib/python3.8/site-packages (from selectivesearch) (1.24.4)\n",
      "Collecting scikit-image (from selectivesearch)\n",
      "  Downloading scikit_image-0.21.0-cp38-cp38-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: scipy>=1.8 in /Users/h383kim/miniforge3/envs/env/lib/python3.8/site-packages (from scikit-image->selectivesearch) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /Users/h383kim/miniforge3/envs/env/lib/python3.8/site-packages (from scikit-image->selectivesearch) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /Users/h383kim/miniforge3/envs/env/lib/python3.8/site-packages (from scikit-image->selectivesearch) (10.3.0)\n",
      "Collecting imageio>=2.27 (from scikit-image->selectivesearch)\n",
      "  Downloading imageio-2.35.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->selectivesearch)\n",
      "  Downloading tifffile-2023.7.10-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting PyWavelets>=1.1.1 (from scikit-image->selectivesearch)\n",
      "  Downloading PyWavelets-1.4.1-cp38-cp38-macosx_11_0_arm64.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: packaging>=21 in /Users/h383kim/miniforge3/envs/env/lib/python3.8/site-packages (from scikit-image->selectivesearch) (24.1)\n",
      "Collecting lazy_loader>=0.2 (from scikit-image->selectivesearch)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Downloading scikit_image-0.21.0-cp38-cp38-macosx_12_0_arm64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.35.1-py3-none-any.whl (315 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.4/315.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading PyWavelets-1.4.1-cp38-cp38-macosx_11_0_arm64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tifffile-2023.7.10-py3-none-any.whl (220 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: selectivesearch\n",
      "  Building wheel for selectivesearch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for selectivesearch: filename=selectivesearch-0.4-py3-none-any.whl size=4334 sha256=27048e2df9829c094845b5357f68a8ae45e112ed2ad33874483eff26793a7be4\n",
      "  Stored in directory: /Users/h383kim/Library/Caches/pip/wheels/66/4e/88/6de23ce74be839a953498c4ebdfa809ad7da9422ac89ae856c\n",
      "Successfully built selectivesearch\n",
      "Installing collected packages: tifffile, PyWavelets, lazy_loader, imageio, scikit-image, selectivesearch\n",
      "Successfully installed PyWavelets-1.4.1 imageio-2.35.1 lazy_loader-0.4 scikit-image-0.21.0 selectivesearch-0.4 tifffile-2023.7.10\n"
     ]
    }
   ],
   "source": [
    "!pip install selectivesearch\n",
    "import selectivesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "52a17751-2d3b-42d3-9339-13b270bc404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "#import selective_search\n",
    "\n",
    "import cv2\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca0b4b1f-c895-482f-b715-e4fa771ecc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def ss_config(ss, img, mode):\n",
    "    ss.setBaseImage(img)\n",
    "\n",
    "    if mode == 's':\n",
    "        ss.switchToSingleStrategy()\n",
    "    elif mode == 'f':\n",
    "        ss.switchToSelectiveSearchFast()\n",
    "    elif mode == 'q':\n",
    "        ss.switchToSelectiveSearchQuality()\n",
    "    else:\n",
    "        print(\"Re-enter the mode. s or f or q\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def selective_search(img, mode='q'):\n",
    "    # Initiate Selective Search\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    # Configure the mode and image\n",
    "    ss_config(ss, img, mode)\n",
    "\n",
    "    # Process Selective-Search\n",
    "    bboxes = ss.process() # bboxes: listof [x, y, w, h]\n",
    "    bboxes[:, 2] += bboxes[:, 0] # bboxes -> listof [x, y, x + w, h]\n",
    "    bboxes[:, 3] += bboxes[:, 1] # bboxes -> listof [x, y, x + w, h + y]\n",
    "\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4172800c-4d0f-49d7-8d3c-3870d2073014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_boxes(root):\n",
    "    bboxes=[]\n",
    "    for obj in root.findall('object'):\n",
    "        cls_name = obj.find('name').text\n",
    "        cls_idx = VOC_CLASSES.index(cls_name)\n",
    "        xmin = int(obj.find('bndbox').find('xmin').text)\n",
    "        ymin = int(obj.find('bndbox').find('ymin').text)\n",
    "        xmax = int(obj.find('bndbox').find('xmax').text)\n",
    "        ymax = int(obj.find('bndbox').find('ymax').text)\n",
    "        bboxes.append([xmin, ymin, ymin, ymax, cls_idx]) \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6c9e6078-63fa-492f-b874-3d9179d9011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxA, boxB):\n",
    "    # Compute the Intersection over Union (IoU) between two bounding boxes\n",
    "    xA = max(boxA[0], boxB[0])  # max of xmin\n",
    "    yA = max(boxA[1], boxB[1])  # max of ymin\n",
    "    xB = min(boxA[2], boxB[2])  # min of xmax\n",
    "    yB = min(boxA[3], boxB[3])  # min of ymax\n",
    "\n",
    "    # Compute the area of the intersection rectangle\n",
    "    interWidth = max(0, xB - xA + 1)\n",
    "    interHeight = max(0, yB - yA + 1)\n",
    "    interArea = interWidth * interHeight\n",
    "\n",
    "    # Compute the area of both bounding boxes\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    # Compute the Intersection over Union by dividing the intersection area by the sum of both areas minus the intersection\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea) if interArea > 0 else 0.0\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9e57f6fe-f35e-402e-9454-675a59aad1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CLASSES = [\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', \n",
    "    'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', \n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "701785ca-6a00-48a5-9afd-0e0fa0c55ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, img_set='trainval', transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.img_set = img_set\n",
    "        \n",
    "        self.annotation_path = os.path.join(self.root, f'PASCAL_VOC_{self.img_set}', 'VOCdevkit', 'VOC2007', 'Annotations')\n",
    "        self.img_path = os.path.join(self.root, f'PASCAL_VOC_{self.img_set}', 'VOCdevkit', 'VOC2007', 'JPEGimages')\n",
    "\n",
    "        self.annotations = [os.path.join(self.annotation_path, xml) for xml in sorted(os.listdir(self.annotation_path)) if not xml.startswith('.')]\n",
    "        self.images = [os.path.join(self.img_path, xml) for xml in sorted(os.listdir(self.img_path)) if not xml.startswith('.')]\n",
    "        self.annotations = self.annotations[:10]\n",
    "        self.images = self.images[:10]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image using Pillow\n",
    "        image = Image.open(self.images[idx])\n",
    "        image = np.array(image)\n",
    "        # Load annotation\n",
    "        tree = ET.parse(self.annotations[idx])\n",
    "        gt_bboxes_labels = parse_xml_boxes(tree.getroot())\n",
    "        gt_bboxes_labels = np.array(gt_bboxes_labels)\n",
    "        bboxes, labels= gt_bboxes_labels[:, :4], gt_bboxes_labels[:, -1]\n",
    "        # Apply transformation if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Convert data to tensors\n",
    "        bboxes = torch.tensor(bboxes).float()\n",
    "        labels = torch.tensor(labels).long()\n",
    "        \n",
    "        return image, bboxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cd3818bd-a4a2-4e96-b22d-4580a1b85cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import roi_pool\n",
    "\n",
    "vgg = models.vgg16_bn(weights=\"IMAGENET1K_V1\", progress=True)\n",
    "\n",
    "class FastRCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        # vgg = models.vgg16(weights=\"IMAGENET1K_V1\")\n",
    "        # Convolutional layers\n",
    "        self.features = vgg.features\n",
    "        # ROI Pooling\n",
    "        self.roi = roi_pool\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            # nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        self.softmax = nn.Linear(4096, num_classes + 1)\n",
    "        self.bbox = nn.Linear(4096, num_classes * 4)\n",
    "        \n",
    "    def forward(self, images, ROIs):\n",
    "        # Compute feature maps\n",
    "        feature_maps = self.features(images)\n",
    "        #print(feature_maps.shape)\n",
    "        # Apply ROI pooling\n",
    "        pooled_features = self.roi(feature_maps, ROIs, output_size=(7, 7), spatial_scale=1.0/16.0)\n",
    "        #print(pooled_features.shape)\n",
    "        # Flatten pooled features\n",
    "        x = pooled_features.view(pooled_features.size(0), -1) # .size(0) = Batch Size(N=64)\n",
    "        # x: [64, 512, 7, 7]\n",
    "        # Classifier\n",
    "        x = self.classifier(x) # x: [64, 4096]\n",
    "        \n",
    "        # Outputs\n",
    "        cls_score = self.softmax(x) # (64, num_classes + 1)\n",
    "        bbox_pred = self.bbox(x) # (64, num_classes * 4)\n",
    "        \n",
    "        return cls_score, bbox_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4a00ba65-aadb-4711-b913-b4974ae586ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, lambda_reg=1):\n",
    "        super().__init__()\n",
    "        self.lambda_reg = lambda_reg\n",
    "    \n",
    "    def forward(self, cls_preds, bbox_preds, labels, bbox_targets, pos_indices, num_classes=20):\n",
    "        # Cross entropy loss for classification\n",
    "        classification_loss = F.cross_entropy(cls_preds, labels)\n",
    "        # Bounding box loss\n",
    "        '''\n",
    "        bbox_preds: [64, 20, 4]\n",
    "        bbox_targets: [len(pos_indices), 4]\n",
    "        '''\n",
    "        if len(pos_indices) > 0:\n",
    "            pos_labels = labels[labels < 20] # Non-background\n",
    "            bbox_preds = bbox_preds.view(-1, num_classes, 4) # Reshaping bbox_preds from [64, num_classes*4] to [64, num_classes, 4]\n",
    "            bbox_preds_pos = bbox_preds[pos_indices, pos_labels, :]\n",
    "            regression_loss = F.smooth_l1_loss(bbox_preds_pos, bbox_targets)\n",
    "        else:\n",
    "            regression_loss = torch.tensor(0.0, requires_grad=True).to(bbox_preds.device)\n",
    "        \n",
    "        totalLoss = classification_loss + regression_loss\n",
    "        return totalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7a75bfa4-dde6-40c0-9d97-2b7dc2f5ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root_dir = '/Users/h383kim/pytorch/data'\n",
    "# Create dataset and dataloader\n",
    "train_dataset = CustomDataset(root_dir, img_set=\"trainval\", transform=None)\n",
    "test_dataset = CustomDataset(root_dir, img_set=\"test\", transform=None)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "56209873-cbd7-4d46-93e9-cead6e3cce9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 10,  10,  10],\n",
       "         [ 13,  13,  13],\n",
       "         [ 10,  10,  10],\n",
       "         ...,\n",
       "         [163, 188, 195],\n",
       "         [162, 187, 194],\n",
       "         [161, 186, 193]],\n",
       " \n",
       "        [[  3,   4,   6],\n",
       "         [  7,   9,   8],\n",
       "         [ 11,  11,  13],\n",
       "         ...,\n",
       "         [163, 188, 195],\n",
       "         [163, 188, 195],\n",
       "         [163, 188, 195]],\n",
       " \n",
       "        [[ 45,  49,  52],\n",
       "         [ 24,  28,  29],\n",
       "         [ 15,  16,  20],\n",
       "         ...,\n",
       "         [165, 190, 197],\n",
       "         [165, 190, 197],\n",
       "         [165, 190, 197]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 31,  12,   8],\n",
       "         [ 36,  17,  11],\n",
       "         [ 29,  10,   3],\n",
       "         ...,\n",
       "         [ 58,  72, 101],\n",
       "         [ 61,  72, 104],\n",
       "         [ 58,  71, 103]],\n",
       " \n",
       "        [[ 36,   9,   2],\n",
       "         [ 34,   7,   0],\n",
       "         [ 38,   8,   0],\n",
       "         ...,\n",
       "         [ 66,  80, 109],\n",
       "         [ 70,  83, 115],\n",
       "         [ 70,  83, 115]],\n",
       " \n",
       "        [[ 49,  17,   6],\n",
       "         [ 60,  22,  11],\n",
       "         [ 73,  25,  13],\n",
       "         ...,\n",
       "         [ 64,  79, 108],\n",
       "         [ 68,  83, 112],\n",
       "         [ 69,  84, 113]]], dtype=uint8),\n",
       " tensor([[263., 211., 211., 339.],\n",
       "         [165., 264., 264., 372.],\n",
       "         [  5., 244., 244., 374.],\n",
       "         [241., 194., 194., 299.],\n",
       "         [277., 186., 186., 220.]]),\n",
       " tensor([8, 8, 8, 8, 8]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8794b04b-4c9c-4361-aeed-96b752713fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "fastrcnn = FastRCNN(20).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8aab8a91-1dec-4ca0-8f3b-c6390067c795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d0ee9a30-5db4-444c-8e89-72a57346c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_transform(proposal, gt_box):\n",
    "    g_xmin, g_ymin, g_w, g_h = gt_box[0], gt_box[1], gt_box[2] - gt_box[0], gt_box[3] - gt_box[1]\n",
    "    p_xmin, p_ymin, p_w, p_h = proposal[0], proposal[1], proposal[2] - proposal[0], proposal[3] - proposal[1]\n",
    "    # G_x, G_y, P_x, P_y = Center coordinates of gt_box and proposal\n",
    "    G_x, G_y = g_xmin + 0.5*g_w, g_ymin + 0.5*g_h\n",
    "    P_x, P_y = p_xmin + 0.5*p_w, p_ymin + 0.5*p_h\n",
    "    # target delats(transformations) to be learned\n",
    "    t_x = (G_x - P_x) / p_w\n",
    "    t_y = (G_y - P_y) / p_h\n",
    "    t_w = torch.log(g_w / p_w)\n",
    "    t_h = torch.log(g_h / p_h)\n",
    "\n",
    "    return torch.tensor([t_x, t_y, t_w, t_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "512041b4-eae7-4dd2-9b2f-dcd861bb984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss(img):\n",
    "    # Move the tensor to CPU and convert to NumPy array\n",
    "    img = img.cpu().numpy()\n",
    "    \n",
    "    labels, regions = selectivesearch.selective_search(img, scale=150, sigma=0.8, min_size=20)\n",
    "    bboxes = [region['rect'] for region in regions]\n",
    "    bboxes = np.array(bboxes)\n",
    "    bboxes[:, 2] += bboxes[:, 0]\n",
    "    bboxes[:, 3] += bboxes[:, 1]\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c9582472-3e8f-4fb3-91bd-ba7913c1a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_proposals(proposals, gt_boxes, gt_labels):\n",
    "    threshold_up = 0.5\n",
    "    threshold_low = 0.1\n",
    "    POS_NUM = 16\n",
    "    NEG_NUM = 48\n",
    "    positive_proposals, positive_labels, negative_proposals = [], [], []\n",
    "    bbox_targets = [] # the target transformation from proposal to gt_bbox\n",
    "    \n",
    "    for proposal in proposals:\n",
    "        best_iou = 0\n",
    "        best_label = None\n",
    "        best_gt_box = None\n",
    "        \n",
    "        for gt_box, gt_label in zip(gt_boxes, gt_labels):\n",
    "            iou = compute_iou(proposal, gt_box)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_label = gt_label\n",
    "                best_gt_box = gt_box\n",
    "        # If the proposal has IOU > 0.5\n",
    "        if best_iou > threshold_up:\n",
    "            positive_proposals.append(proposal)\n",
    "            positive_labels.append(best_label)\n",
    "            # Compute regression targets\n",
    "            bbox_target = bbox_transform(proposal, best_gt_box)\n",
    "            bbox_targets.append(bbox_target)\n",
    "#        # Else if the proposal has 0.1 < IOU <= 0.5\n",
    "#        elif best_iou > threshold_low:\n",
    "#            negative_proposals.append(proposal)\n",
    "#            # Doesn't need to keep track of label, because background label is fixed\n",
    "#        else:\n",
    "#            pass\n",
    "        else:\n",
    "            negative_proposals.append(proposal)\n",
    "            \n",
    "    # Choose ~16 positives and 48 negatives (More negatives if postives is less than 16)\n",
    "    if len(positive_proposals) < POS_NUM:\n",
    "        POS_NUM = len(positive_proposals)\n",
    "        NEG_NUM = (64 - POS_NUM)\n",
    "    \n",
    "    pos_indices = random.sample(range(len(positive_proposals)), POS_NUM)\n",
    "    neg_indices = random.sample(range(len(negative_proposals)), NEG_NUM)\n",
    "    # Sampling takes place (Replacing the object and all its references if any)\n",
    "    positive_proposals[:] = [positive_proposals[i] for i in pos_indices]\n",
    "    positive_labels[:] = [positive_labels[i] for i in pos_indices]\n",
    "    bbox_targets[:] = [bbox_targets[i] for i in pos_indices]\n",
    "    negative_proposals[:] = [negative_proposals[i] for i in neg_indices]\n",
    "    \n",
    "    total_proposals = positive_proposals + negative_proposals\n",
    "    total_labels = positive_labels + [20] * NEG_NUM # Background class label as 20\n",
    "    total_labels = torch.tensor(total_labels, dtype=torch.int64)\n",
    "    if len(bbox_targets) > 0:\n",
    "        bbox_targets = torch.stack(bbox_targets)\n",
    "    else:\n",
    "        bbox_targets = torch.tensor(bbox_targets)\n",
    "    return total_proposals, total_labels, bbox_targets, pos_indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "776204a7-7995-49b2-a74c-82eda48c69c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    train_loss, correct, train_acc = 0, 0, 0\n",
    "    total_proposals = 0\n",
    "    \n",
    "    #for epoch in range(num_epochs):\n",
    "    for image, boxes, labels in dataloader:\n",
    "        # OpenCV version selective search\n",
    "        np_img = np.squeeze(np.array(image))\n",
    "        ROIs = selective_search(np_img)\n",
    "        image = image.float().to(DEVICE)\n",
    "        boxes = boxes.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # Generate ROIs and choose 64 proposals(16 positive + 48 negative)\n",
    "        # # pip install selectivesearch version\n",
    "        # squeezed_img = torch.squeeze(image)\n",
    "        # ROIs = ss(squeezed_img)\n",
    "        '''\n",
    "        ROIs: total proposals (positive + negative)\n",
    "        ROI_labels: labels of total proposals\n",
    "        bbox_targets: target transformations for positive proposals only\n",
    "        pos_indices: Indices of positive proposals to be used in bbox regression loss\n",
    "        '''\n",
    "        boxes = boxes.reshape(-1, 4)\n",
    "        labels = labels.flatten()\n",
    "        ROIs, ROI_labels, bbox_targets, pos_indices = sample_proposals(ROIs, boxes, labels)\n",
    "        ROIs = torch.tensor(np.array(ROIs)).to(DEVICE) # Converting first into np.array then to Tensor for spped (list -> np.array -> tensor)\n",
    "        ROI_labels = ROI_labels.to(DEVICE)\n",
    "        bbox_targets = bbox_targets.to(DEVICE)\n",
    "        pos_indices = torch.tensor(pos_indices).to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        image = image.permute(0, 3, 1, 2)\n",
    "        # ROIs is of shape [K, 4] (without batch index) and batch size is 1\n",
    "        batch_indices = torch.zeros((ROIs.size(0), 1), device=ROIs.device)  # Batch index 0 for all ROIs\n",
    "        ROIs_with_batch_index = torch.cat([batch_indices, ROIs], dim=1)  # Concatenate batch index with ROIs\n",
    "        # ROIs_with_batch_index will have shape [K, 5]\n",
    "        cls_scores, bbox_preds = model(image, ROIs_with_batch_index)\n",
    "\n",
    "        # Compute loss\n",
    "        '''\n",
    "        cls_scores : [64, num_classes + 1]\n",
    "        bbox_preds : [64, num_classes * 4]\n",
    "        ROI_labels : [64, 1]\n",
    "        bbox_targets : [len(pos_indices), 4]\n",
    "        '''\n",
    "        loss = loss_fn(cls_scores, bbox_preds, ROI_labels, bbox_targets, pos_indices)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = torch.argmax(cls_scores, dim=1)\n",
    "        correct += pred.eq(ROI_labels.view_as(pred)).sum().item()\n",
    "        total_proposals += 64\n",
    "    \n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc = 100. * correct / total_proposals\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0d6b4373-d57c-479b-b4a2-98b1135f0277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "loss_lst, acc_lst = [], []\n",
    "def fine_tune(model: torch.nn.Module, \n",
    "              train_dataloader: torch.utils.data.DataLoader, \n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              loss_fn: torch.nn.Module=MultiTaskLoss(),\n",
    "              num_epochs: int=10):\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start_time = time.time()\n",
    "        # Train the model and print save the results\n",
    "        train_loss, train_acc = train(model=model,\n",
    "                                      dataloader=train_dataloader, \n",
    "                                      optimizer=optimizer,\n",
    "                                      loss_fn=loss_fn)\n",
    "        # Storing loss and acc for the epoch\n",
    "        loss_lst.append(train_loss)\n",
    "        acc_lst.append(train_acc)\n",
    "        \n",
    "        if train_acc > best_acc:\n",
    "            best_acc = train_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        end_time = time.time()\n",
    "        time_elapsed = end_time - start_time\n",
    "        print(f\"------------ epoch {epoch} ------------\")\n",
    "        print(f\"Train loss: {train_loss:.4f} | Train acc: {train_acc:.2f}%\")\n",
    "        print(f\"Time taken: {time_elapsed / 60:.0f}min {time_elapsed % 60:.0f}s\")\n",
    "        \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a8ef8762-b904-4177-a097-60f996b0e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([\n",
    "    {'params': fastrcnn.features.parameters(), 'lr':1e-4},\n",
    "    {'params': fastrcnn.classifier.parameters(), 'lr':1e-3},\n",
    "    {'params': fastrcnn.softmax.parameters(), 'lr':1e-3},\n",
    "    {'params': fastrcnn.bbox.parameters(), 'lr':1e-3},\n",
    "], momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8460afd8-7833-45f1-83d0-dcf2064c98db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/h383kim/miniforge3/envs/env/lib/python3.8/site-packages/skimage/feature/texture.py:353: UserWarning: Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present. It is recommended to use this function with images of integer dtype.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ epoch 1 ------------\n",
      "Train loss: 3.0049 | Train acc: 50.47%\n",
      "Time taken: 4min 45s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fastrcnn_FineTuned \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfastrcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[102], line 16\u001b[0m, in \u001b[0;36mfine_tune\u001b[0;34m(model, train_dataloader, optimizer, loss_fn, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model and print save the results\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                              \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Storing loss and acc for the epoch\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss_lst\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[101], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03mcls_scores : [64, num_classes + 1]\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mbbox_preds : [64, num_classes * 4]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mROI_labels : [64, 1]\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mbbox_targets : [len(pos_indices), 4]\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mROI_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[93], line 22\u001b[0m, in \u001b[0;36mMultiTaskLoss.forward\u001b[0;34m(self, cls_preds, bbox_preds, labels, bbox_targets, pos_indices, num_classes)\u001b[0m\n\u001b[1;32m     20\u001b[0m     regression_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msmooth_l1_loss(bbox_preds_pos, bbox_targets)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     regression_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m totalLoss \u001b[38;5;241m=\u001b[39m classification_loss \u001b[38;5;241m+\u001b[39m regression_loss\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m totalLoss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fastrcnn_FineTuned = fine_tune(fastrcnn, train_dataloader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8587c72-fa8d-4e96-9bde-75abaac6eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Loss Graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_lst)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.title(\"Train Loss Graph\")\n",
    "\n",
    "# Train Accuracy Graph\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(acc_lst)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Train Accuracy (%)\")\n",
    "plt.title(\"Train Accuracy Graph\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
